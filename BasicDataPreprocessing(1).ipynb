{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["This notebook contains code related to some basic data preprocessing and data cleaning. It analyses the existing dataframe and then make chnages and save in a new file."],"metadata":{"id":"Hx_A4mdgk9Ze"}},{"cell_type":"code","execution_count":17,"metadata":{"id":"7UL92SZQk3HC","executionInfo":{"status":"ok","timestamp":1742762668605,"user_tz":-330,"elapsed":19,"user":{"displayName":"Jyotsana Verma","userId":"07404936933751180998"}}},"outputs":[],"source":["import pandas as pd\n","\n","def load_and_print_data(file_path):\n","\n","    # Load the dataset\n","    df = pd.read_csv(file_path)\n","\n","    # Print the dataframe before pre-processing\n","    print(\"Dataframe before pre-processing:\")\n","    print(df.head())\n","\n","    return df\n"]},{"cell_type":"code","source":["# Example usage\n","file_path = \"/content/insurance_claims.csv\"\n","df_old = load_and_print_data(file_path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d0MC-jMhFgFl","executionInfo":{"status":"ok","timestamp":1742762675335,"user_tz":-330,"elapsed":48,"user":{"displayName":"Jyotsana Verma","userId":"07404936933751180998"}},"outputId":"4b613291-2829-49b7-e497-8b05abebfc5c"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Dataframe before pre-processing:\n","   months_as_customer  age  policy_number policy_bind_date policy_state  \\\n","0                 328   48         521585       2014-10-17           OH   \n","1                 228   42         342868       2006-06-27           IN   \n","2                 134   29         687698       2000-09-06           OH   \n","3                 256   41         227811       1990-05-25           IL   \n","4                 228   44         367455       2014-06-06           IL   \n","\n","  policy_csl  policy_deductable  policy_annual_premium  umbrella_limit  \\\n","0    250/500               1000                1406.91               0   \n","1    250/500               2000                1197.22         5000000   \n","2    100/300               2000                1413.14         5000000   \n","3    250/500               2000                1415.74         6000000   \n","4   500/1000               1000                1583.91         6000000   \n","\n","   insured_zip  ... witnesses police_report_available total_claim_amount  \\\n","0       466132  ...         2                     YES              71610   \n","1       468176  ...         0                       ?               5070   \n","2       430632  ...         3                      NO              34650   \n","3       608117  ...         2                      NO              63400   \n","4       610706  ...         1                      NO               6500   \n","\n","  injury_claim property_claim  vehicle_claim  auto_make auto_model auto_year  \\\n","0         6510          13020          52080       Saab        92x      2004   \n","1          780            780           3510   Mercedes       E400      2007   \n","2         7700           3850          23100      Dodge        RAM      2007   \n","3         6340           6340          50720  Chevrolet      Tahoe      2014   \n","4         1300            650           4550     Accura        RSX      2009   \n","\n","  fraud_reported  \n","0              Y  \n","1              Y  \n","2              N  \n","3              Y  \n","4              N  \n","\n","[5 rows x 39 columns]\n"]}]},{"cell_type":"code","source":["def preprocess_insurance_data(df):\n","\n","    # Replace '?' with NaN for proper handling of missing values\n","    df.replace('?', pd.NA, inplace=True)\n","\n","    # Convert categorical target variable to binary (Y -> 1, N -> 0)\n","    df['fraud_reported'] = df['fraud_reported'].map({'Y': 1, 'N': 0})\n","\n","    # Convert date columns to datetime format\n","    df['policy_bind_date'] = pd.to_datetime(df['policy_bind_date'], errors='coerce')\n","    df['incident_date'] = pd.to_datetime(df['incident_date'], errors='coerce')\n","\n","    # Drop unnecessary columns\n","    drop_columns = ['policy_number', 'insured_zip', 'incident_location', 'auto_make', 'auto_model']\n","    df.drop(columns=drop_columns, inplace=True, errors='ignore')\n","\n","    # Handle missing values by replacing categorical NaNs with 'Unknown'\n","    categorical_cols = df.select_dtypes(include=['object']).columns\n","    df[categorical_cols] = df[categorical_cols].fillna('Unknown')\n","\n","    # Encode categorical variables using one-hot encoding\n","    df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)\n","\n","    return df\n"],"metadata":{"id":"ZBGQqtncocRL","executionInfo":{"status":"ok","timestamp":1742762677823,"user_tz":-330,"elapsed":14,"user":{"displayName":"Jyotsana Verma","userId":"07404936933751180998"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["# Example usage\n","df_cleaned = preprocess_insurance_data(df_old)\n","\n","# Print the dataframe after pre processing\n","print(\"\\nDataframe after pre-processing:\")\n","print(df_cleaned.head())\n","\n","# Save the preprocessed dataset\n","df_cleaned.to_csv(\"insurance_claims_preprocessed.csv\", index=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XddDuQ1lnwwD","executionInfo":{"status":"ok","timestamp":1742762680739,"user_tz":-330,"elapsed":81,"user":{"displayName":"Jyotsana Verma","userId":"07404936933751180998"}},"outputId":"2690260a-f389-4f86-e77b-a9833e2e1614"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Dataframe after pre-processing:\n","   months_as_customer  age policy_bind_date  policy_deductable  \\\n","0                 328   48       2014-10-17               1000   \n","1                 228   42       2006-06-27               2000   \n","2                 134   29       2000-09-06               2000   \n","3                 256   41       1990-05-25               2000   \n","4                 228   44       2014-06-06               1000   \n","\n","   policy_annual_premium  umbrella_limit  capital-gains  capital-loss  \\\n","0                1406.91               0          53300             0   \n","1                1197.22         5000000              0             0   \n","2                1413.14         5000000          35100             0   \n","3                1415.74         6000000          48900        -62400   \n","4                1583.91         6000000          66000        -46000   \n","\n","  incident_date  incident_hour_of_the_day  ...  incident_city_Columbus  \\\n","0    2015-01-25                         5  ...                    True   \n","1    2015-01-21                         8  ...                   False   \n","2    2015-02-22                         7  ...                    True   \n","3    2015-01-10                         5  ...                   False   \n","4    2015-02-17                        20  ...                   False   \n","\n","   incident_city_Hillsdale  incident_city_Northbend  incident_city_Northbrook  \\\n","0                    False                    False                     False   \n","1                    False                    False                     False   \n","2                    False                    False                     False   \n","3                    False                    False                     False   \n","4                    False                    False                     False   \n","\n","   incident_city_Riverwood  incident_city_Springfield  \\\n","0                    False                      False   \n","1                     True                      False   \n","2                    False                      False   \n","3                    False                      False   \n","4                    False                      False   \n","\n","   property_damage_Unknown  property_damage_YES  \\\n","0                    False                 True   \n","1                     True                False   \n","2                    False                False   \n","3                     True                False   \n","4                    False                False   \n","\n","   police_report_available_Unknown  police_report_available_YES  \n","0                            False                         True  \n","1                             True                        False  \n","2                            False                        False  \n","3                            False                        False  \n","4                            False                        False  \n","\n","[5 rows x 96 columns]\n"]}]},{"cell_type":"markdown","source":["The changes made to the dataset,\n","1. **Handling Missing Values:**  \n","   - Replaced `?` with `NaN` for proper identification of missing data.  \n","   - Filled missing categorical values with `'Unknown'`.  \n","\n","2. **Target Variable Conversion:**  \n","   - Converted `fraud_reported` from categorical (`Y/N`) to binary (`1/0`).  \n","\n","3. **Date Format Conversion:**  \n","   - Converted `policy_bind_date` and `incident_date` into proper `datetime` format.  \n","\n","4. **Dropped Unnecessary Columns:**  \n","   - Removed `policy_number`, `insured_zip`, `incident_location`, `auto_make`, and `auto_model` as they were unlikely to contribute to fraud detection.  \n","\n","5. **Encoding Categorical Variables:**  \n","   - Applied one-hot encoding to categorical columns while avoiding dummy variable traps by dropping the first category.  \n","\n","6. **Final Dataset Preparation:**  \n","   - Processed dataset was saved as `insurance_claims_preprocessed.csv` for further model training.  \n"],"metadata":{"id":"BTMIUr_emXGj"}},{"cell_type":"code","source":["# Handling outliers\n","import numpy as np\n","\n","# Function to handle outliers using IQR method\n","def handle_outliers_iqr(df, col):\n","    Q1 = df[col].quantile(0.25)\n","    Q3 = df[col].quantile(0.75)\n","    IQR = Q3 - Q1\n","    lower_bound = Q1 - 1.5 * IQR\n","    upper_bound = Q3 + 1.5 * IQR\n","\n","    # Removing outliers\n","    df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]\n","    return df\n","\n","# Function to handle outliers by capping (Winsorization)\n","def handle_outliers_capping(df, col):\n","    lower_bound = df[col].quantile(0.05)\n","    upper_bound = df[col].quantile(0.95)\n","\n","    # Capping the outliers\n","    df[col] = np.where(df[col] < lower_bound, lower_bound, df[col])\n","    df[col] = np.where(df[col] > upper_bound, upper_bound, df[col])\n","    return df\n","\n","# Function to handle outliers by log transformation\n","def handle_outliers_log(df, col):\n","    df[col] = np.log1p(df[col])  # log1p to avoid log(0)\n","    return df\n","\n","# Applying IQR method to extreme value columns\n","df_cleaned = handle_outliers_iqr(df_cleaned, 'policy_annual_premium')\n","df_cleaned = handle_outliers_iqr(df_cleaned, 'total_claim_amount')\n","\n","df_cleaned = handle_outliers_capping(df_cleaned, 'injury_claim')\n","df_cleaned = handle_outliers_capping(df_cleaned, 'property_claim')\n","df_cleaned = handle_outliers_capping(df_cleaned, 'vehicle_claim')\n","\n","df_cleaned = handle_outliers_log(df_cleaned, 'capital-gains')\n","df_cleaned = handle_outliers_log(df_cleaned, 'capital-loss')\n","\n","# Displaying the final dataframe after outlier handling\n","print(df_cleaned.describe())\n","\n","# Saving the updated dataframe to a new CSV file\n","df_cleaned.to_csv('insurance_claims_outliers_handled.csv', index=False)\n"],"metadata":{"id":"HhpFnk2hRVQY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1742762684411,"user_tz":-330,"elapsed":121,"user":{"displayName":"Jyotsana Verma","userId":"07404936933751180998"}},"outputId":"d3bea1f0-a935-4af4-c458-7e67f4455292"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["       months_as_customer         age               policy_bind_date  \\\n","count          990.000000  990.000000                            990   \n","mean           204.355556   38.954545  2002-02-13 10:03:38.181818240   \n","min              0.000000   19.000000            1990-01-08 00:00:00   \n","25%            117.250000   32.000000            1995-09-23 18:00:00   \n","50%            200.000000   38.000000            2002-04-01 12:00:00   \n","75%            276.750000   44.000000            2008-04-22 12:00:00   \n","max            479.000000   64.000000            2015-02-22 00:00:00   \n","std            114.638399    9.120969                            NaN   \n","\n","       policy_deductable  policy_annual_premium  umbrella_limit  \\\n","count         990.000000             990.000000    9.900000e+02   \n","mean         1137.878788            1256.002960    1.103030e+06   \n","min           500.000000             617.110000   -1.000000e+06   \n","25%           500.000000            1090.402500    0.000000e+00   \n","50%          1000.000000            1257.200000    0.000000e+00   \n","75%          2000.000000            1413.045000    0.000000e+00   \n","max          2000.000000            1896.910000    1.000000e+07   \n","std           612.004568             235.364343    2.301688e+06   \n","\n","       capital-gains  capital-loss                  incident_date  \\\n","count     990.000000         471.0                            990   \n","mean        5.315012           0.0  2015-01-30 09:28:43.636363776   \n","min         0.000000           0.0            2015-01-01 00:00:00   \n","25%         0.000000           0.0            2015-01-15 00:00:00   \n","50%         0.000000           0.0            2015-01-31 00:00:00   \n","75%        10.841559           0.0            2015-02-15 00:00:00   \n","max        11.517923           0.0            2015-03-01 00:00:00   \n","std         5.400581           0.0                            NaN   \n","\n","       incident_hour_of_the_day  number_of_vehicles_involved  bodily_injuries  \\\n","count                990.000000                   990.000000       990.000000   \n","mean                  11.645455                     1.841414         0.993939   \n","min                    0.000000                     1.000000         0.000000   \n","25%                    6.000000                     1.000000         0.000000   \n","50%                   12.000000                     1.000000         1.000000   \n","75%                   17.000000                     3.000000         2.000000   \n","max                   23.000000                     4.000000         2.000000   \n","std                    6.929849                     1.019577         0.819359   \n","\n","        witnesses  total_claim_amount  injury_claim  property_claim  \\\n","count  990.000000          990.000000    990.000000      990.000000   \n","mean     1.483838        52710.111111   7369.828283     7295.515152   \n","min      0.000000          100.000000    460.000000      459.000000   \n","25%      0.250000        41737.500000   4310.000000     4435.000000   \n","50%      1.000000        58055.000000   6775.000000     6745.000000   \n","75%      2.000000        70507.500000  11265.000000    10875.000000   \n","max      3.000000       112320.000000  15660.000000    15540.000000   \n","std      1.112136        26310.011906   4696.868336     4561.980221   \n","\n","       vehicle_claim    auto_year  fraud_reported  \n","count     990.000000   990.000000      990.000000  \n","mean    37645.025253  2005.107071        0.246465  \n","min      3284.500000  1995.000000        0.000000  \n","25%     30257.500000  2000.000000        0.000000  \n","50%     42140.000000  2005.000000        0.000000  \n","75%     50750.000000  2010.000000        0.000000  \n","max     62838.000000  2015.000000        1.000000  \n","std     18277.558422     6.009821        0.431170  \n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n","  result = getattr(ufunc, method)(*inputs, **kwargs)\n"]}]},{"cell_type":"markdown","source":["On analysing the dataset we realised that the columns such as `months_as_customer` , `age` , `policy_annual_premium` ,  `capital-gains / capital-loss` , `total_claim_amount` , `injury_claim` , `property_claim` , `vehicle_claim` could behve as outliers.\n","So, we used  different approaches for different types of columns that are as follows :\n","\n","    IQR Method: For columns where outliers seem like data errors (policy premium).\n","\n","    Capping (95th Percentile): For injury, property, and vehicle claims where extreme values are valid but need limiting.\n","\n","    Log Transformation: For capital gains/loss to normalize extreme skewness."],"metadata":{"id":"bwqijKnEtFU0"}}]}